{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementation of decoding methods from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-15T15:50:57.278055Z",
     "iopub.status.busy": "2025-08-15T15:50:57.277798Z",
     "iopub.status.idle": "2025-08-15T15:50:59.189012Z",
     "shell.execute_reply": "2025-08-15T15:50:59.188215Z",
     "shell.execute_reply.started": "2025-08-15T15:50:57.278030Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-15T15:50:59.190722Z",
     "iopub.status.busy": "2025-08-15T15:50:59.189993Z",
     "iopub.status.idle": "2025-08-15T15:51:08.166624Z",
     "shell.execute_reply": "2025-08-15T15:51:08.165889Z",
     "shell.execute_reply.started": "2025-08-15T15:50:59.190690Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-15 15:51:03.395601: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1755273063.418433     117 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1755273063.425310     117 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 1280)\n",
       "    (wpe): Embedding(1024, 1280)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-35): 36 x GPT2Block(\n",
       "        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D(nf=3840, nx=1280)\n",
       "          (c_proj): Conv1D(nf=1280, nx=1280)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=5120, nx=1280)\n",
       "          (c_proj): Conv1D(nf=1280, nx=5120)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1280, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model_name = 'gpt2-large'\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.padding_side = 'left' # required for parallelism (default is set to right padding for some reason)\n",
    "\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-15T15:51:08.169590Z",
     "iopub.status.busy": "2025-08-15T15:51:08.168906Z",
     "iopub.status.idle": "2025-08-15T15:51:08.174604Z",
     "shell.execute_reply": "2025-08-15T15:51:08.173765Z",
     "shell.execute_reply.started": "2025-08-15T15:51:08.169563Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "# end of sequence token is the end of document token in GPT-2 \n",
    "eod_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-15T15:51:08.175779Z",
     "iopub.status.busy": "2025-08-15T15:51:08.175514Z",
     "iopub.status.idle": "2025-08-15T15:51:08.207239Z",
     "shell.execute_reply": "2025-08-15T15:51:08.206518Z",
     "shell.execute_reply.started": "2025-08-15T15:51:08.175756Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "prompt = \"The quick brown fox\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors='pt').to(device)\n",
    "\n",
    "outputs = model(**inputs)\n",
    "logits = outputs.logits\n",
    "\n",
    "last_logits = logits[-1][-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Greedy decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-15T15:51:08.489398Z",
     "iopub.status.busy": "2025-08-15T15:51:08.489140Z",
     "iopub.status.idle": "2025-08-15T15:51:08.511434Z",
     "shell.execute_reply": "2025-08-15T15:51:08.510687Z",
     "shell.execute_reply.started": "2025-08-15T15:51:08.489360Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' jumps'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_id = torch.argmax(last_logits, dim=-1)\n",
    "tokenizer.decode(token_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pure sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-15T15:51:08.512517Z",
     "iopub.status.busy": "2025-08-15T15:51:08.512230Z",
     "iopub.status.idle": "2025-08-15T15:51:08.663358Z",
     "shell.execute_reply": "2025-08-15T15:51:08.662514Z",
     "shell.execute_reply.started": "2025-08-15T15:51:08.512490Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' jumps'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = F.softmax(last_logits, dim=0)\n",
    "idx = probs.multinomial(num_samples=1)\n",
    "tokenizer.decode(idx) # idx is the token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top-k sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-15T15:51:08.842852Z",
     "iopub.status.busy": "2025-08-15T15:51:08.842592Z",
     "iopub.status.idle": "2025-08-15T15:51:08.848793Z",
     "shell.execute_reply": "2025-08-15T15:51:08.848096Z",
     "shell.execute_reply.started": "2025-08-15T15:51:08.842828Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " jumps\n",
      " jumped\n",
      " jump\n",
      ",\n",
      " gets\n",
      " leapt\n",
      " is\n",
      " leaps\n",
      " was\n",
      " jumping\n"
     ]
    }
   ],
   "source": [
    "logits_k = torch.topk(last_logits, 10, dim=-1)\n",
    "\n",
    "for token_id in logits_k.indices:\n",
    "    print(tokenizer.decode(token_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-15T16:05:25.669694Z",
     "iopub.status.busy": "2025-08-15T16:05:25.669139Z",
     "iopub.status.idle": "2025-08-15T16:05:25.675451Z",
     "shell.execute_reply": "2025-08-15T16:05:25.674867Z",
     "shell.execute_reply.started": "2025-08-15T16:05:25.669671Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' is'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    probs = F.softmax(logits_k.values, dim=0)\n",
    "    idx = logits_k.values.multinomial(num_samples=1)\n",
    "    \n",
    "tokenizer.decode(logits_k.indices[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nucleus sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-15T15:52:58.455839Z",
     "iopub.status.busy": "2025-08-15T15:52:58.455585Z",
     "iopub.status.idle": "2025-08-15T15:52:58.465609Z",
     "shell.execute_reply": "2025-08-15T15:52:58.465048Z",
     "shell.execute_reply.started": "2025-08-15T15:52:58.455822Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' jumps'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    probs = F.softmax(last_logits, dim=0)\n",
    "    sorted_probs, sorted_indices = torch.sort(probs, descending=True)\n",
    "    cumul_probs = torch.cumsum(sorted_probs, dim=0)\n",
    "\n",
    "    p=0.9 # Top-p with p=0.9\n",
    "    for i in range(sorted_probs.size()[0]):\n",
    "        if cumul_probs[i]>p:\n",
    "            sorted_probs = sorted_probs[:i]\n",
    "            break\n",
    "    \n",
    "    sorted_probs = sorted_probs/sum(sorted_probs)\n",
    "    idx = sorted_probs.multinomial(num_samples=1)\n",
    "    token_id = sorted_indices[idx]\n",
    "\n",
    "tokenizer.decode(token_id)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
